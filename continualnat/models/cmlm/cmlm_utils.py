from typing import List, Tuple, Union

import pandas as pd
import torch
from tabulate import tabulate
from transformers import MBartTokenizer, MBartTokenizerFast


def tabulate_mask_predict_steps(
    token_ids_steps: torch.Tensor,
    tokenizer: Union[MBartTokenizer, MBartTokenizerFast],
    skip_special_tokens: bool = False,
) -> Tuple[List[str], List[pd.DataFrame]]:
    """
    Tabuletes the tokens generated by the mask-predict algorithm such that they can be printed in a prettier way.
    :param token_ids_steps: tensor of generated tokens at each mask-predict step for a batch of sentences to
        translate, its shape is (bsz, length_beam_size, iterations + 1, max_tgt_len).
    :param tokenizer: the tokenizer used in order to remove the pad tokens.
    :param skip_special_tokens: whether to skip the special tokens while convert the token ids into tokens
        (default=False).
    :return: tuple containing a list of prettified strings containing the steps performed by mask-predict for each
        translated sentence and a list of dataframes on which the strings are built on.
    """
    tokens_at_each_step = []
    for token_ids_sentence in token_ids_steps:
        tokens_step = []
        for i, token_ids in enumerate(token_ids_sentence):
            if i == 0 and skip_special_tokens:
                # the first step is always entirely masked so there is no reason to tabulate an empty list of tokens
                continue

            tokens_sentence = tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)
            tokens_sentence_no_pad = [token for token in tokens_sentence if token != tokenizer.pad_token]
            tokens_step.append(tokens_sentence_no_pad)

        tokens_at_each_step.append(tokens_step)

    df_tokens = [pd.DataFrame(tokens_steps) for tokens_steps in tokens_at_each_step]
    df_tokens_tabulate = [tabulate(df_step) for df_step in df_tokens]
    return df_tokens_tabulate, df_tokens
