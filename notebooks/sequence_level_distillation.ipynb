{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sequence-level knowledge distillation\n",
    "We perform sequence-level knowledge distillation on an en->de dataset with a model provided by Helsinki-NLP from https://huggingface.co/Helsinki-NLP."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import all the libraries and set up the device and the parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utilities import model_n_parameters, model_size\n",
    "\n",
    "# Parameters\n",
    "src_lang = \"en\"\n",
    "tgt_lang = \"de\"\n",
    "lang_pair = src_lang + \"-\" + tgt_lang\n",
    "dataset = \"yhavinga/ccmatrix\"\n",
    "dataset_size = 100\n",
    "cache_dir = \"D:/MasterDegreeThesis/datasets/ccmatrix\"\n",
    "batch_size = 16\n",
    "evaluate_teacher = True\n",
    "\n",
    "# Set-up device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Working on: {0}\".format(device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the models and its tokenizer\n",
    "The chosen model is based on the transformer base while the tokenizer is based on Sentencepiece."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 74410496\n",
      "Model trainable parameters: 73886208\n",
      "Model size in mb: 284.0751533508301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opus_mt_model: MarianMTModel = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-{0}\"\n",
    "                                                             .format(lang_pair)).to(device)\n",
    "opus_mt_tokenizer: MarianTokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-{0}\"\n",
    "                                                                     .format(lang_pair))\n",
    "model_parameters, model_trainable_parameters = model_n_parameters(opus_mt_model)\n",
    "opus_mt_size = model_size(opus_mt_model)\n",
    "print(\"Model parameters: {0}\\nModel trainable parameters: {1}\".format(model_parameters, model_trainable_parameters))\n",
    "print(\"Model size in mb: {0}\\n\".format(opus_mt_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the dataset and extract target sentences\n",
    "In this example we will work with some sentence pairs from the ccmatrix dataset available at https://huggingface.co/datasets/yhavinga/ccmatrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ccmatrix (D:/MasterDegreeThesis/datasets/ccmatrix_en_de/yhavinga___ccmatrix/en-de/1.0.0/5f733aeea277b2b1bb792442ba120c0f7f4b1c7288897051bdf1e9865fe77b93)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_to_distill = load_dataset(dataset, lang_pair, cache_dir=\"{0}_{1}_{2}\".format(cache_dir, src_lang, tgt_lang),\n",
    "                                  split=\"train[:{0}]\".format(dataset_size), ignore_verifications=True)\n",
    "\n",
    "# Extract target sentences\n",
    "tgt_sentences = [tgt_sentence[tgt_lang] for tgt_sentence in dataset_to_distill[\"translation\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compute translations\n",
    "Use the model to predict tokens by using the integrated generator on top of the model which uses beam search with $beam\\_size=4$, the tokens will be detokenized in order to build the distilled sentences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens prediction: 100%|██████████| 7/7 [00:33<00:00,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En sentence: It is “before the Lord,” and therefore it ought to be before us.\n",
      "De target sentence: Es ist \"vor dem Herrn\", und deshalb sollte es vor uns sein.\n",
      "De generated by the model: Es ist vor dem Herrn, und deshalb sollte es vor uns sein.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the dataloader\n",
    "dataloader_opus_mt = DataLoader(dataset_to_distill[\"translation\"], batch_size=batch_size)\n",
    "translations = []\n",
    "for batch in tqdm(dataloader_opus_mt, \"Tokens prediction\"):\n",
    "    # Retrieve sentence in the source language\n",
    "    src_batch = batch[src_lang]\n",
    "\n",
    "    # Tokenized the batch and generate predictions, this will be the most time-consuming part\n",
    "    batch_tokens = opus_mt_tokenizer(src_batch, padding=True, return_tensors=\"pt\").to(device)\n",
    "    output = opus_mt_model.generate(**batch_tokens, max_new_tokens=opus_mt_model.config.max_length)\n",
    "\n",
    "    # Detokenize the model's output to obtain translations\n",
    "    translation = opus_mt_tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    translations.append(translation)\n",
    "\n",
    "# Reconstruct the translations' list by dissolving the batches\n",
    "translations = [translation for batch in translations for translation in batch]\n",
    "\n",
    "print(\"En sentence: {0}\".format(dataset_to_distill[\"translation\"][12][\"en\"]))\n",
    "print(\"De target sentence: {0}\".format(tgt_sentences[12]))\n",
    "print(\"De generated by the model: {0}\".format(translations[12]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate teacher performance\n",
    "We can also evaluate the performance of the teacher model on the dataset that it has just distilled by computing BLEU and chrF scores."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                teacher_model lang_pair  dataset_size       bleu       chrf\n",
      "0  Helsinki-NLP/opus-mt-en-de     en-de           100  51.141876  71.473842\n"
     ]
    }
   ],
   "source": [
    "# Evaluate teacher's translations and save scores inside a csv file\n",
    "if evaluate_teacher:\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    chrf_metric = evaluate.load(\"chrf\")\n",
    "    bleu_score = bleu_metric.compute(predictions=translations, references=tgt_sentences)[\"bleu\"] * 100\n",
    "    chrf_score = chrf_metric.compute(predictions=translations, references=tgt_sentences)[\"score\"]\n",
    "    df_scores = {\n",
    "        \"teacher_model\": \"Helsinki-NLP/opus-mt-{0}\".format(lang_pair),\n",
    "        \"lang_pair\": lang_pair,\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"bleu\": [bleu_score],\n",
    "        \"chrf\": [chrf_score]\n",
    "    }\n",
    "    df_scores = pd.DataFrame(df_scores)\n",
    "    if os.path.exists(\"../data/distillation_teacher_scores.csv\"):\n",
    "        df_teacher_scores: pd.DataFrame = pd.read_csv(\"../data/distillation_teacher_scores.csv\", index_col=0)\n",
    "        df_teacher_scores = pd.concat([df_teacher_scores, df_scores], ignore_index=True)\n",
    "        df_teacher_scores = df_teacher_scores.drop_duplicates([\"teacher_model\", \"lang_pair\", \"dataset_size\"])\n",
    "        df_teacher_scores.to_csv(\"../data/distillation_teacher_scores.csv\")\n",
    "    else:\n",
    "        df_scores.to_csv(\"../data/distillation_teacher_scores.csv\")\n",
    "\n",
    "    print(df_scores)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the translations\n",
    "As a final step, perhaps the most important one, we need to save the translations in order to use them as the new ground truth fot the student model. We advise to publish the dataset on https://huggingface.co/ in order to let other users to work with it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Save translations\n",
    "with open(\"../data/distilled_dataset_{0}_{1}.txt\".format(src_lang, tgt_lang), \"w\", encoding=\"utf_8\") as datafile:\n",
    "    for translation in translations:\n",
    "        datafile.write(translation + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
