{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training a tokenizer\n",
    "Train a sentencepiece BPE tokenizer from scratch using the Huggingface's tokenizers package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and concatenate the datasets\n",
    "Load the dataset needed to train the tokenizer, in our case we will be using English, French, German and Spanish sentences from the cc100 dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_en = load_dataset(\"cc100\", lang=\"en\", split=\"train\",\n",
    "                          cache_dir=\"/disk1/a.ristori/cc100\", verification_mode=\"no_checks\", streaming=True)\n",
    "dataset_de = load_dataset(\"cc100\", lang=\"de\", split=\"train\",\n",
    "                          cache_dir=\"/disk1/a.ristori/cc100\", verification_mode=\"no_checks\", streaming=True)\n",
    "dataset_fr = load_dataset(\"cc100\", lang=\"fr\", split=\"train\",\n",
    "                          cache_dir=\"/disk1/a.ristori/cc100\", verification_mode=\"no_checks\", streaming=True)\n",
    "dataset_es = load_dataset(\"cc100\", lang=\"es\", split=\"train\",\n",
    "                          cache_dir=\"/disk1/a.ristori/cc100\", verification_mode=\"no_checks\", streaming=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There should be an equal amount of sentences for each language."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_samples = 1000000\n",
    "dataset_en = dataset_en.take(num_samples)\n",
    "dataset_de = dataset_de.take(num_samples)\n",
    "dataset_fr = dataset_fr.take(num_samples)\n",
    "dataset_es = dataset_es.take(num_samples)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can finally concatenate the datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets([dataset_en, dataset_de, dataset_fr, dataset_es])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the tokenizer\n",
    "We will build a tokenizer with a shared vocab of size 32000."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers.implementations import SentencePieceBPETokenizer\n",
    "from tokenizers.processors import ByteLevel\n",
    "\n",
    "def batch_iterator(batch_size):\n",
    "    batch = []\n",
    "    for example in dataset:\n",
    "        batch.append(example[\"text\"])\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "    if batch:  # yield last batch\n",
    "        yield batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the special tokens and the vocab size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<length>\", \"<mask>\"]\n",
    "vocab_size = 32000\n",
    "sentencepiece_tokenizer = SentencePieceBPETokenizer()\n",
    "sentencepiece_tokenizer.post_processor = ByteLevel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the tokenizer (this will take a while depending on the number of sentences in your dataset) and save its configuration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentencepiece_tokenizer.train_from_iterator(batch_iterator(1000), vocab_size, special_tokens=special_tokens)\n",
    "sentencepiece_tokenizer.save(f\"sentencepiece_config_{vocab_size / 1000}k.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use your tokenizer\n",
    "Load your tokenizer and work with it alongside huggingface's transformers library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizerFast\n",
    "# The previously trained tokenizer can only work with the fast version of the hugginface tokenizers\n",
    "tokenizer = MBartTokenizerFast(tokenizer_file=\"tokenizers/sp_32k.json\", cls_token=\"<length>\",\n",
    "                                   src_lang=\"en_XX\", tgt_lang=\"de_DE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
