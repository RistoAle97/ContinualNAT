# Model
model:
  name: "Transformer" # this should reflect the model's class that you want to train
  parameters:
    d_model: 512
    n_heads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    dim_ff: 2048
    dropout: 0.1
    layer_norm_eps: float = 1e-6

# Tokenizer
tokenizer: "facebook/mbart-large-cc25"
padding: "longest"

# Dataset
dataset:
  path: "yhavinga/ccmatrix"
  name: "en-de"
  cache_dir: "/disk1/a.ristori/datasets/ccmatrix"

# Training parameters
src_lang: "en"
tgt_lang: "de"
batch_size: 256
max_length: 128
tokens_per_batch: 4096
accumulate_gradient: False
training_updates: 100000
log_step: 100

# Scheduler
warmup_steps: 4000
