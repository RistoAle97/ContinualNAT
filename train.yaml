# Model
model:
  name: "Transformer" # this should reflect the model's class that you want to train
  parameters:
    d_model: 512
    n_heads: 8
    num_encoder_layers: 6
    num_decoder_layers: 6
    dim_ff: 2048
    dropout: 0.1
    layer_norm_eps: float = 1e-6
    norm_first: True
    shared_embeddings_src_trg: True
    shared_embeddings_trg_out: True

# Tokenizer
tokenizer: "facebook/mbart-large-cc25"
padding: "longest"

# Dataset
dataset:
  path: "wmt14"
  name: "de-en"
  cache_dir: "/disk1/a.ristori/datasets/wmt14"

# Training parameters
src_lang: "en"
tgt_lang: "de"
batch_size: 32
max_length: 64
tokens_per_batch: 25000
accumulation_steps: None
training_updates: 100000
log_step: 100
validation_step: True

# Loss function
label_smoothing: 0.1

# Scheduler
warmup_steps: 4000
